{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybP9ZJxXSuAT"
      },
      "source": [
        "<font size=10>**Retrieval Augmented Generation (RAG)**</font>\n",
        "\n",
        "<font size=6>**Salesforce CRM Q&A Assistant**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elRO0CX2KxTg"
      },
      "source": [
        "## <font color='blue'>**Solution Approach**</font>\n",
        "\n",
        "<font color='blue'>**1. Data Ingestion and Chunking**</font>\n",
        "* Install necessary libraries and load the CRM documents from a local folder using **LangChain’s** `PyPDFLoader` with TextLoader.\n",
        "* Apply RecursiveCharacterTextSplitter to break the documents into overlapping text chunks while preserving semantic context.\n",
        "\n",
        "<font color='blue'>**2. Embedding & Vector Store Creation**</font>\n",
        "* Generate text embeddings using OpenAIEmbeddings to convert text chunks into vector representations.\n",
        "* Create a vector store using Chroma, and persist it locally using .persist() for reusability.\n",
        "\n",
        "<font color='blue'>**3. Query Handling & Answer Generation**</font>\n",
        "* Retrieve top-k most relevant document chunks from Chroma using semantic similarity search.\n",
        "* Feed the retrieved context and user query into a custom prompt template using LangChain’s LLMChain with ChatOpenAI.\n",
        "* Generate a final answer based on the prompt + context and strictly grounded in the provided context.\n",
        "\n",
        "<font color='blue'>**4. LLM-Based Multi-Metric Evaluation (MLS)**</font>\n",
        "* Use a second LLMChain to evaluate the generated response using five key evaluation metrics:\n",
        "  * **Groundedness –** Is the answer supported by retrieved context?\n",
        "\n",
        "  * **Relevance –** Does the answer align with the user’s query?\n",
        "\n",
        "  * **Faithfulness –** Are the statements logically valid and consistent?\n",
        "\n",
        "  * **Context Precision –** Does the answer avoid including irrelevant context?\n",
        "\n",
        "  * **Context Recall –** Has it captured all important info from context?\n",
        "\n",
        "* Each metric is scored on a scale of 1–5 and accompanied by a justification generated by the LLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2xeNeW3S2z3"
      },
      "source": [
        "## **Business Questions for Evaluation**\n",
        "\n",
        "The RAG system is designed to embed and store large documentation sets, conduct vector similarity search, and generate accurate answers using a Large Language Model (LLM) with clear provenance of the retrieved context.\n",
        "\n",
        "Here are example questions that the internal sales team frequently asks, which the RAG assistant should be able to answer accurately:\n",
        "\n",
        "*1. What are the key features and functionalities of Salesforce CRM's Sales Cloud module?*\n",
        "\n",
        "*2. What are the steps to integrate third-party marketing tools with Salesforce CRM?*\n",
        "\n",
        "*3. How do I generate a report showing lead conversion rates by region?*\n",
        "\n",
        "*4. How can we track and report on customer satisfaction in Salesforce?*\n",
        "\n",
        "*5. Where can I find historical engagement data for a specific customer?*\n",
        "\n",
        "*6. What is the best way to segment customers for an upcoming campaign*?\n",
        "\n",
        "*7. What are the best practices for updating opportunities during a sales review?*\n",
        "\n",
        "*8. Can we automate lead assignment based on geography or product interest?*\n",
        "\n",
        "These questions are intentionally business-critical, ensuring the RAG system is tested for both completeness and precision in responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "⚠️ Note:\n",
        "All cell outputs have been intentionally cleared to protect sensitive\n",
        "and proprietary data. This repository focuses on demonstrating\n",
        "architecture, evaluation methodology, and implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beqUFNomT_E3"
      },
      "source": [
        "# **<font color='blue'>Library Installation and OpenAI LLM Calling</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WWQwwZ67qkC",
        "outputId": "e7d28d6d-a6c5-4760-803b-8989c5e960a7"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install langchain_community langchain chromadb pypdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KjQ65jAF-Y9q"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import requests # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7_6KYlT-bKy",
        "outputId": "8f71af65-4435-41e6-da1f-d4e68e54378c"
      },
      "outputs": [],
      "source": [
        "# Load the JSON file and extract values\n",
        "#create a config.json file with your API key for this chunk to work\n",
        "file_name = 'config.json'\n",
        "with open(file_name, 'r') as file:\n",
        "    config = json.load(file)\n",
        "    API_KEY = config.get(\"API_KEY\") # Loading the API Key\n",
        "    OPENAI_API_BASE = config.get(\"OPENAI_API_BASE\") # Loading the API Base Url\n",
        "\n",
        "model_name = \"gpt-4o-mini\"\n",
        "\n",
        "# Storing API credentials in environment variables\n",
        "os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "os.environ[\"OPENAI_BASE_URL\"] = OPENAI_API_BASE\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Create a chat completion\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
        "        {\"role\": \"user\", \"content\": \"Hello, can you return an executive summary of multiple pdfs? I need them for my meeting. \"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Print the assistant's reply\n",
        "print(completion.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jyrMs9olDW8"
      },
      "source": [
        "# **<font color='blue'>Data Ingestion and Chunking</font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1_q3CzSsH84"
      },
      "source": [
        "**This section loads all Salesforce-related PDF documents and chunks them into manageable sizes for LLM input.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ5GhazY-cvq",
        "outputId": "9827a910-f327-45e4-f4c3-d6826cbeb6d6"
      },
      "outputs": [],
      "source": [
        "! unzip \"sample_pdfs.zip\" -d ./sample_pdfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BSs0vdGB-ibE"
      },
      "outputs": [],
      "source": [
        "# Uploading multiple pdfs:\n",
        "from glob import glob\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Path to folder with PDFs\n",
        "DOC_FOLDER = \"Salesforce/\"\n",
        "pdf_files = glob(DOC_FOLDER + \"*.pdf\")  # grabs all PDFs in the folder\n",
        "\n",
        "all_pages = []\n",
        "\n",
        "# Load each PDF and extract pages\n",
        "for pdf_path in pdf_files:\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    pages = loader.load()\n",
        "    all_pages.extend(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLzCTBVC-j27",
        "outputId": "c931737f-da5a-4242-c603-7083ded6b5a3"
      },
      "outputs": [],
      "source": [
        "print(pdf_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ndOMyX2a-lL6"
      },
      "outputs": [],
      "source": [
        "# Chunking the data\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # type: ignore\n",
        "\n",
        "\n",
        "# Split the doc into smaller chunks i.e. chunk_size=500\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "\n",
        "chunks = text_splitter.split_documents(all_pages)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxyBr8RjlNLc"
      },
      "source": [
        "# **<font color='blue'>Embedding and Vector Store Creation</font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgI2Os6ftlor"
      },
      "source": [
        "The next step is to convert the above chunks into vector embeddings using OpenAI Embeddings. These embeddings are stored in a Chroma vector database for later retrieval during question answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YnOd2urSAg0q"
      },
      "outputs": [],
      "source": [
        "# Directory to store vector database\n",
        "CHROMA_PATH = \"./chroma_db\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z5h95D0-l_y",
        "outputId": "d6dc2db7-1992-4a1e-f0e5-9cc5f4619005"
      },
      "outputs": [],
      "source": [
        "# Calculate the embeddings and save in database\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings # type: ignore\n",
        "from langchain.vectorstores import Chroma # type: ignore\n",
        "\n",
        "# Get OpenAI Embedding model\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=API_KEY, openai_api_base=OPENAI_API_BASE)\n",
        "\n",
        "# Embed the chunks as vectors and load them into the database\n",
        "db_chroma = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "j32gS437-nLb"
      },
      "outputs": [],
      "source": [
        "# Let us start with the following user query (user_input)\n",
        "\n",
        "user_input= 'How will Sales Cloud improve female representation in our office?.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "rS_ObubGGqFG"
      },
      "outputs": [],
      "source": [
        "# retrieve context - top 5 most relevant (closests) chunks to the query vector\n",
        "# (by default Langchain is using cosine distance metric)\n",
        "\n",
        "docs_chroma = db_chroma.similarity_search_with_score(user_input, k=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4Znm7IvHI0i",
        "outputId": "9015d6df-b568-4e46-e0ca-569d320f03fc"
      },
      "outputs": [],
      "source": [
        "# @title Metadata Display\n",
        "# @markdown For each chunk, it prints: Cleaned content (tab characters removed), File source, Page number\n",
        "\n",
        "\n",
        "for i, (doc, _score) in enumerate(docs_chroma): # unpack the tuple into doc and _score\n",
        "    print(f\"Retrieved chunk {i+1}: \\n\")\n",
        "    print(doc)\n",
        "    print(doc.page_content.replace('\\t', ' '))\n",
        "    print(\"Source: \", doc.metadata['source'],\"\\n \")\n",
        "    print(\"Page Number: \",doc.metadata['page'],\"\\n===================================================== \\n\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMafzjqDaxXB",
        "outputId": "66bc7df6-e6c3-4d13-a875-ff9830309d3b"
      },
      "outputs": [],
      "source": [
        "# Concatenate all the retrieved chunk texts to form a single context_text block\n",
        "context_text = \"\\n\\n\".join([doc.page_content for doc, _score in docs_chroma])\n",
        "\n",
        "# Check how many documents were actually retrieved (should be 5)\n",
        "len(docs_chroma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOec5sxcu4lJ"
      },
      "source": [
        "*This is used as input for the final LLM call to generate an answer.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TceyYEU-lYr7"
      },
      "source": [
        "# **<font color='blue'>Query Handling and Answer Generation</font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owMaO5zcvgaD"
      },
      "source": [
        "Implementation of the response generation step in our RAG pipeline using LangChain and an OpenAI-compatible chat model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ5Sq1dYvpd6"
      },
      "source": [
        "`ChatPromptTemplate` is used for formatting dynamic prompts.\n",
        "\n",
        "`ChatOpenAI` provides a wrapper around OpenAI's chat models *(like gpt-3.5-turbo, gpt-4o-mini, etc.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfpx6rgo-olV",
        "outputId": "6fabfa20-9df7-4fd2-9606-da901c4b6760"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# You can use a prompt template\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are an assistant to a Sales Team. Your task is to summarize and provide relevant information to the team's question based on the provided context.\n",
        "\n",
        "Answer the question based only on the following context:\n",
        "{context}\n",
        "Answer the question based on the above context: {question}.\n",
        "\n",
        "Please adhere to the following guidelines:\n",
        "- Provide a detailed answer.\n",
        "- Don’t justify your answers.\n",
        "- Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
        "- Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
        "- If the answer is not found in the context, it is very very important for you to respond with \"Sorry, this is out of my knowledge base\"\n",
        "\"\"\"\n",
        "\n",
        "# Load retrieved context and user query in the prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "prompt = prompt_template.format(context=context_text, question=user_input)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uixcYVHL-sT6",
        "outputId": "77b8d72c-f5fe-42e8-a61b-79139007fcfe"
      },
      "outputs": [],
      "source": [
        "# @title Model Invocation and Response generation\n",
        "\n",
        "# Call LLM model to generate the answer based on the given context and query\n",
        "model = ChatOpenAI(\n",
        "    model_name=model_name,\n",
        "    openai_api_key=API_KEY,\n",
        "    openai_api_base=OPENAI_API_BASE\n",
        "    )\n",
        "\n",
        "response_text = model.predict(prompt)\n",
        "print(response_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtX7Q-fRxZSB"
      },
      "source": [
        "We print and inspect `context_for_query` to verify what information is being passed into the model prompt for generating outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FTpjxp9QCLzU"
      },
      "outputs": [],
      "source": [
        "# A template string that formats a structured message.\n",
        "\n",
        "user_message_template = \"\"\"\n",
        "###Question\n",
        "{question}\n",
        "\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Answer\n",
        "{answer}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "2EwqaaBVCM9a"
      },
      "outputs": [],
      "source": [
        "user_input = \"What are the key features and functionalities of Salesforce CRM Sales Cloud?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LgQx3Ldwlez"
      },
      "source": [
        "Each item in `docs_chroma` is a tuple: `(document_chunk, similarity_score)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4dZRYrGECTup"
      },
      "outputs": [],
      "source": [
        "# (by default Langchain is using cosine distance metric)\n",
        "docs_chroma = db_chroma.similarity_search_with_score(user_input, k=5)\n",
        "\n",
        "context_list = [d[0].page_content for d in docs_chroma]\n",
        "context_for_query = \". \".join(context_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj8P5FK7wvAr"
      },
      "source": [
        "This extracts the text content from each document in `docs_chroma`, resulting in a list of context chunks as strings.\n",
        "These chunks are then joined into a single string, separated by a period and space, to form the complete context.\n",
        "The combined context is printed to verify the information being passed into the model prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfFEwFXACw6g",
        "outputId": "4f560901-ef8f-44cb-bc64-b65fda7a51ef"
      },
      "outputs": [],
      "source": [
        "print(context_for_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i5ZjPzElpPT"
      },
      "source": [
        "# **<font color='blue'>LLM Based Evaluation</font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-0mvIRlxvE8"
      },
      "source": [
        "## **Groundedness**\n",
        "\n",
        "To assess whether the AI-generated **answer is strictly based on the provided context** without introducing any external or hallucinated information.\n",
        "\n",
        "**Why It Matters:**\n",
        "\n",
        "In Retrieval-Augmented Generation (RAG) systems, hallucination is a major risk. Groundedness evaluation helps ensure that **answers are trustworthy** and **traceable to source documents**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dIYtZpSyAxHF"
      },
      "outputs": [],
      "source": [
        "# @title Metric 1\n",
        "\n",
        "\n",
        "# This prompt effectively turns the LLM into a groundedness evaluator, instructing it to reflect, explain, and then score the answer.\n",
        "\n",
        "groundedness_rater_system_message = \"\"\"\n",
        "You are tasked with rating AI generated answers to questions posed by users.\n",
        "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
        "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
        "\n",
        "Evaluation criteria:\n",
        "The task is to judge the extent to which the metric is followed by the answer.\n",
        "1 - The metric is not followed at all\n",
        "2 - The metric is followed only to a limited extent\n",
        "3 - The metric is followed to a good extent\n",
        "4 - The metric is followed mostly\n",
        "5 - The metric is followed completely\n",
        "\n",
        "Metric:\n",
        "The answer should be derived only from the information presented in the context.\n",
        "\n",
        "Instructions:\n",
        "1. First write down the steps that are needed to evaluate the answer as per the metric.\n",
        "2. Give a step-by-step explanation if the answer adheres to the metric considering the question and context as the input.\n",
        "3. Next, evaluate the extent to which the metric is followed.\n",
        "4. Use the previous information to rate the answer using the evaluaton criteria and assign a score.\n",
        "\n",
        "Output Format:\n",
        "Arrange your output in the following JSON format.\n",
        "{\n",
        "    \"steps\": write down the steps that are needed to evaluate the context as per the metric.\n",
        "    \"explanation\": provide a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
        "    \"evaluation\": the extent to which the metric is followed.\n",
        "    \"rating\": <1,2,3,4,5>\n",
        "}\n",
        "DO NOT output anything else before or after the JSON output.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "nX3ewS-fDui4"
      },
      "outputs": [],
      "source": [
        "groundedness_prompt = [\n",
        "    {'role':'system', 'content': groundedness_rater_system_message},\n",
        "    {'role': 'user', 'content': user_message_template.format(\n",
        "        question=user_input,\n",
        "        context=context_for_query,\n",
        "        answer=response_text\n",
        "        )\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRrke_zFDGT0",
        "outputId": "3eee319c-c494-4756-d013-b81ca8ae667f"
      },
      "outputs": [],
      "source": [
        "groundedness_eval = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=groundedness_prompt,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(groundedness_eval.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlS1h8G80Xkb"
      },
      "source": [
        "## **Relevance**\n",
        "\n",
        "To evaluate **how relevant the AI-generated answer is to the question**, based on the context used.\n",
        "\n",
        "*Relevance means the answer should cover all important aspects of the question without adding unrelated or missing important information.*\n",
        "  - *It shouldn’t go off-topic, nor miss key information.*\n",
        "\n",
        "\n",
        "**Why It Matters:**\n",
        "\n",
        "Even if an answer is factual and grounded, it might not actually address the **user’s intent**.\n",
        "\n",
        "- Relevance ensures answers aren’t off-topic, overly generic, or missing key information.\n",
        "\n",
        "- In RAG, this avoids responses like:\n",
        "\n",
        "  \"*Here is some info about the topic...*\" instead of actually answering the question.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNhh6p7ECLFr"
      },
      "outputs": [],
      "source": [
        "# @title Metric 2\n",
        "\n",
        "\n",
        "# This turns the LLM into a Relevance Rater, ensuring the answer meaningfully covers what the question asked, and nothing more or less.\n",
        "\n",
        "relevance_rater_system_message = \"\"\"\n",
        "You are tasked with rating AI generated answers to questions posed by users.\n",
        "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
        "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
        "\n",
        "Evaluation criteria:\n",
        "The task is to judge the extent to which the metric is followed by the answer.\n",
        "1 - The metric is not followed at all\n",
        "2 - The metric is followed only to a limited extent\n",
        "3 - The metric is followed to a good extent\n",
        "4 - The metric is followed mostly\n",
        "5 - The metric is followed completely\n",
        "\n",
        "Metric:\n",
        "Relevance measures how well the answer addresses the main aspects of the question, based on the context.\n",
        "Consider whether all and only the important aspects are contained in the answer when evaluating relevance.\n",
        "\n",
        "Instructions:\n",
        "1. First write down the steps that are needed to evaluate the context as per the metric.\n",
        "2. Give a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
        "3. Next, evaluate the extent to which the metric is followed.\n",
        "4. Use the previous information to rate the context using the evaluaton criteria and assign a score.\n",
        "\n",
        "Output Format:\n",
        "Arrange your output in the following JSON format.\n",
        "{\n",
        "    \"steps\": write down the steps that are needed to evaluate the context as per the metric.\n",
        "    \"explanation\": provide a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
        "    \"evaluation\": the extent to which the metric is followed.\n",
        "    \"rating\": <1,2,3,4,5>\n",
        "}\n",
        "DO NOT output anything else before or after the JSON output.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnAJAn4gETOQ"
      },
      "outputs": [],
      "source": [
        "relevance_prompt = [\n",
        "    {'role':'system', 'content': relevance_rater_system_message},\n",
        "    {'role': 'user', 'content': user_message_template.format(\n",
        "        question=user_input,\n",
        "        context=context_for_query,\n",
        "        answer=response_text\n",
        "        )\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyM42FdZEdZx",
        "outputId": "762184cc-92a7-42d5-f04a-30cf92c3fc26"
      },
      "outputs": [],
      "source": [
        "# Calling the evaluation API\n",
        "\n",
        "relevance_eval = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=relevance_prompt,\n",
        ")\n",
        "\n",
        "print(relevance_eval.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5jRMIfo0YQ9"
      },
      "source": [
        "## **Faithfulness**\n",
        "\n",
        "To assess whether the **answer contains only claims that are explicitly or implicitly supported** by the provided context (i.e., faithful to source).\n",
        "\n",
        "*`Faithfulness ensures the answer doesn't make things up, distort facts, or draw unjustified conclusions.`*\n",
        "\n",
        "\n",
        "**Why It Matters:**\n",
        "\n",
        "- In **RAG pipelines**, even grounded answers can be **unfaithful** — the LLM might use valid sources but make an incorrect inference.\n",
        "\n",
        "- This metric flags **hallucinations** or **logical overreach** from the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz0RgPkcogSc"
      },
      "outputs": [],
      "source": [
        "# @title Metric 3\n",
        "\n",
        "\n",
        "# This turns the LLM into a faithfulness checker to ensure the generated content doesn’t hallucinate beyond what the context states.\n",
        "\n",
        "faithfulness_rater_system_message = \"\"\"\n",
        "You are tasked with rating AI generated answers to questions posed by users.\n",
        "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
        "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
        "\n",
        "Evaluation criteria:\n",
        "The task is to judge the extent to which the metric is followed by the answer.\n",
        "1 - The metric is not followed at all\n",
        "2 - The metric is followed only to a limited extent\n",
        "3 - The metric is followed to a good extent\n",
        "4 - The metric is followed mostly\n",
        "5 - The metric is followed completely\n",
        "\n",
        "Metric:\n",
        "The answer can be directly inferred based on the context.\n",
        "\n",
        "Instructions:\n",
        "1. First write down the steps that are needed to evaluate the context as per the metric.\n",
        "2. Give a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
        "3. Next, evaluate the extent to which the metric is followed.\n",
        "4. Use the previous information to rate the context using the evaluaton criteria and assign a score.\n",
        "\n",
        "Output Format:\n",
        "Arrange your output in the following JSON format.\n",
        "{\n",
        "    \"steps\": write down the steps that are needed to evaluate the context as per the metric.\n",
        "    \"explanation\": provide a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
        "    \"evaluation\": the extent to which the metric is followed.\n",
        "    \"rating\": <1,2,3,4,5>\n",
        "}\n",
        "DO NOT output anything else before or after the JSON output.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4I29-IWpgqb"
      },
      "outputs": [],
      "source": [
        "faithfulness_prompt = [\n",
        "    {'role':'system', 'content': faithfulness_rater_system_message},\n",
        "    {'role': 'user', 'content': user_message_template.format(\n",
        "        question=user_input,\n",
        "        context=context_for_query,\n",
        "        answer=response_text\n",
        "        )\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bnysbS9pm7a",
        "outputId": "f9ea50d1-a8b2-4f34-ffac-7a9c8ee7efa4"
      },
      "outputs": [],
      "source": [
        "# Invoke the LLM model to perform faithfulness evaluation\n",
        "\n",
        "faithfulness_eval = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=faithfulness_prompt,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(faithfulness_eval.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHDBilWn0ZON"
      },
      "source": [
        "## **Context Precision**\n",
        "\n",
        "To assess **whether the context was actually useful** in helping the AI generate its answer.\n",
        "\n",
        "\n",
        "**What It Measures:**\n",
        "\n",
        "Determines whether the **retrieved context** was actually **used** to construct the answer.\n",
        "\n",
        "1. `High score` = Answer is tightly derived from the context\n",
        "\n",
        "2. `Low score` = Context was either irrelevant or ignored\n",
        "\n",
        "\n",
        "**Why It Matters:**\n",
        "\n",
        "Even if the answer is correct, if the **context wasn't used**, then the RAG pipeline is being **inefficient or misleading**.\n",
        "\n",
        "- Helps diagnose over-retrieval or irrelevant chunking\n",
        "\n",
        "- Improves **contextual alignment** in future retrievals\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ynUe4DwtMkd"
      },
      "outputs": [],
      "source": [
        "# @title Metric 4\n",
        "\n",
        "\n",
        "# This prompt essentially turns the LLM into a “context usage checker,” ensuring that irrelevant or unused context doesn’t sneak in.\n",
        "\n",
        "context_precision_rater_system_message = \"\"\"\n",
        "You are tasked with rating AI generated answers to questions posed by users.\n",
        "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
        "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
        "\n",
        "Evaluation criteria:\n",
        "The task is to judge the extent to which the metric is followed by the answer.\n",
        "1 - The metric is not followed at all\n",
        "2 - The metric is followed only to a limited extent\n",
        "3 - The metric is followed to a good extent\n",
        "4 - The metric is followed mostly\n",
        "5 - The metric is followed completely\n",
        "\n",
        "Metric:\n",
        "Verify if the context was useful in arriving at the given answer.\n",
        "\n",
        "Instructions:\n",
        "1. First write down the steps that are needed to evaluate the context as per the metric.\n",
        "2. Give a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
        "3. Next, evaluate the extent to which the metric is followed.\n",
        "4. Use the previous information to rate the context using the evaluaton criteria and assign a score.\n",
        "\n",
        "Output Format:\n",
        "Arrange your output in the following JSON format.\n",
        "{\n",
        "    \"steps\": write down the steps that are needed to evaluate the context as per the metric.\n",
        "    \"explanation\": provide a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
        "    \"evaluation\": the extent to which the metric is followed.\n",
        "    \"rating\": <1,2,3,4,5>\n",
        "}\n",
        "DO NOT output anything else before or after the JSON output.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jLZ7sb4tv_R"
      },
      "outputs": [],
      "source": [
        "context_precision_prompt = [\n",
        "    {'role':'system', 'content': context_precision_rater_system_message},\n",
        "    {'role': 'user', 'content': user_message_template.format(\n",
        "        question=user_input,\n",
        "        context=context_for_query,\n",
        "        answer=response_text\n",
        "        )\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNzlCdHktxL7",
        "outputId": "ed23ab94-739f-44a1-adbe-b0ca0c2dade2"
      },
      "outputs": [],
      "source": [
        "context_precision_eval = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=context_precision_prompt,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(context_precision_eval.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZhABfyi0Z-c"
      },
      "source": [
        "## **Context Recall**\n",
        "\n",
        "It evaluates not just whether the context was *used*, but whether **each sentence in the answer** can be **traced back** to something present in the context.\n",
        "\n",
        "\n",
        "**Why It Matters:**\n",
        "\n",
        "- High scores mean **low hallucination risk**\n",
        "\n",
        "- Low scores highlight answers with **extra information not grounded in retrieved docs**\n",
        "\n",
        "This metric is key in **evaluating trustworthiness and factual accuracy** in RAG systems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrYFtrzwuBdJ"
      },
      "outputs": [],
      "source": [
        "# @title Metric 5\n",
        "\n",
        "\n",
        "# This is the system instruction prompt provided to the evaluator LLM. It sets the rules for evaluating Context Recall.\n",
        "\n",
        "context_recall_rater_system_message = \"\"\"\n",
        "You are tasked with rating AI generated answers to questions posed by users.\n",
        "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
        "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
        "\n",
        "Evaluation criteria:\n",
        "The task is to judge the extent to which the metric is followed by the answer.\n",
        "1 - The metric is not followed at all\n",
        "2 - The metric is followed only to a limited extent\n",
        "3 - The metric is followed to a good extent\n",
        "4 - The metric is followed mostly\n",
        "5 - The metric is followed completely\n",
        "\n",
        "Metric:\n",
        "Analyze each sentence in the answer and classify if the sentence can be attributed to the given context or not.\n",
        "\n",
        "Instructions:\n",
        "1. First write down the steps that are needed to evaluate the context as per the metric.\n",
        "2. Give a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
        "3. Next, evaluate the extent to which the metric is followed.\n",
        "4. Use the previous information to rate the context using the evaluaton criteria and assign a score.\n",
        "\n",
        "Output Format:\n",
        "Arrange your output in the following JSON format.\n",
        "{\n",
        "    \"steps\": write down the steps that are needed to evaluate the context as per the metric.\n",
        "    \"explanation\": provide a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
        "    \"evaluation\": the extent to which the metric is followed.\n",
        "    \"rating\": <1,2,3,4,5>\n",
        "}\n",
        "DO NOT output anything else before or after the JSON output.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riyz_jFABOeO"
      },
      "source": [
        "This ensures the LLM receives all necessary information and follows the context recall rubric for scoring.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmAsUOUtuDqo"
      },
      "outputs": [],
      "source": [
        "context_recall_prompt = [\n",
        "    {'role':'system', 'content': context_recall_rater_system_message},\n",
        "    {'role': 'user', 'content': user_message_template.format(\n",
        "        question=user_input,\n",
        "        context=context_for_query,\n",
        "        answer=response_text\n",
        "        )\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0sdGkwiBU9d"
      },
      "source": [
        "This helps you see exactly which answer segments are traceable to context and which are not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBx7Av71uFRR",
        "outputId": "43f32eea-e02b-4953-8c67-e6072360600b"
      },
      "outputs": [],
      "source": [
        "context_recall_eval = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=context_recall_prompt,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(context_recall_eval.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLvkMbSzlx67"
      },
      "source": [
        "# **<font color='blue'>Inference and Output Generation</font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRF4drn8CXzR"
      },
      "source": [
        "The purpose of this section is to collect all the **evaluation outputs** from each metric (groundedness, relevance, etc.), compile them into a **single pandas DataFrame**, and render both raw data and a **clean Markdown version** of the model response for user inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_fJbSm2JeEg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17LSDI1ig5xu"
      },
      "outputs": [],
      "source": [
        "# @title Load Evaluation Results and show the ouputs\n",
        "\n",
        "\n",
        "\n",
        "# Each *_eval.choices[0].message.content contains the model's structured rating (steps, explanation, score).\n",
        "# .strip() cleans up extra whitespace.\n",
        "\n",
        "# Get the full raw content from each evaluation\n",
        "groundedness_output = groundedness_eval.choices[0].message.content.strip()\n",
        "relevance_output = relevance_eval.choices[0].message.content.strip()\n",
        "faithfulness_output = faithfulness_eval.choices[0].message.content.strip()\n",
        "context_precision_output = context_precision_eval.choices[0].message.content.strip()\n",
        "context_recall_output = context_recall_eval.choices[0].message.content.strip()\n",
        "\n",
        "# Build the DataFrame row with full JSON strings\n",
        "row = {\n",
        "    \"query\": user_input,\n",
        "    \"response\": response_text,\n",
        "    \"groundedness_evaluation\": groundedness_output,\n",
        "    \"relevance_evaluation\": relevance_output,\n",
        "    \"faithfulness_evaluation\": faithfulness_output,\n",
        "    \"context_precision_evaluation\": context_precision_output,\n",
        "    \"context_recall_evaluation\": context_recall_output\n",
        "}\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vOIfU8pDEJ8"
      },
      "source": [
        "* `pd.DataFrame([row])`: Wraps the dictionary into a one-row DataFrame\n",
        "\n",
        "* `display.expand_frame_repr = True`: Prevents output truncation when printing wide tables\n",
        "\n",
        "* `max_colwidth = 300`: Ensures large JSON fields aren't cut off mid-way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77EJVL9mC8Yd"
      },
      "outputs": [],
      "source": [
        "# Ensure full JSON is shown, not truncated\n",
        "pd.set_option('display.expand_frame_repr', True)\n",
        "pd.set_option('display.max_colwidth', 300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJdEJPjyDSmO"
      },
      "source": [
        "This final step:\n",
        "\n",
        "* Displays the entire evaluation **DataFrame** (one row per query)\n",
        "\n",
        "* Then, renders the model’s `response` in **Markdown** for better readability (preserving lists, headings, formatting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "3ffTaGF0DAed",
        "outputId": "67b0ffc0-a6f7-45da-a7f4-439cf24d750e"
      },
      "outputs": [],
      "source": [
        "# Display\n",
        "display(df)\n",
        "display(Markdown(f\"### Response Generated \\n\\n{df.loc[0, 'response']}\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (Algorithms-Kernel)",
      "language": "python",
      "name": "algorithms-kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
